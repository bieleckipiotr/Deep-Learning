{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, utils\n",
    "import time\n",
    "import numpy as np\n",
    "data_dir = './data/cinic-10_image_classification_challenge-dataset/train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(image_path, label):\n",
    "    # Load image\n",
    "    image = tf.io.read_file(image_path)\n",
    "    # Decode PNG image to tensor\n",
    "    image = tf.image.decode_png(image, channels=3)  # Adjust channels according to your images\n",
    "    # Normalize pixel values to range [0, 1]\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def preprocess_data():\n",
    "\n",
    "    class_names = os.listdir(data_dir)\n",
    "\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for class_name in class_names:\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        for image_name in os.listdir(class_dir):\n",
    "            image_path = os.path.join(class_dir, image_name)\n",
    "            image_paths.append(image_path)\n",
    "            labels.append(class_names.index(class_name))\n",
    "\n",
    "    # Create TensorFlow Dataset from the loaded data\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "    dataset = dataset.map(load_and_preprocess_image)\n",
    "\n",
    "    dataset = dataset.shuffle(buffer_size=10000)\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = int(0.2 * len(dataset))\n",
    "\n",
    "    train_dataset = dataset.take(train_size)\n",
    "    test_dataset = dataset.skip(train_size).take(test_size)\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=len(image_paths)).batch(32)\n",
    "    test_dataset = test_dataset.shuffle(buffer_size=len(image_paths)).batch(32)\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 90000 files belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = utils.image_dataset_from_directory(\n",
    "    \"./data/cinic-10_image_classification_challenge-dataset/train\",\n",
    "    image_size=(32, 32),\n",
    "    label_mode=\"categorical\",\n",
    "    seed=123,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.experimental.preprocessing.Resizing(224, 224, interpolation=\"bilinear\", input_shape=(32, 32, 3)))\n",
    "model.add(layers.Conv2D(96, 11, strides=4, padding='same'))\n",
    "model.add(layers.Lambda(tf.nn.local_response_normalization))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.MaxPooling2D(3, strides=2))\n",
    "model.add(layers.Conv2D(256, 5, strides=4, padding='same'))\n",
    "model.add(layers.Lambda(tf.nn.local_response_normalization))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.MaxPooling2D(3, strides=2))\n",
    "model.add(layers.Conv2D(384, 3, strides=4, padding='same'))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Conv2D(384, 3, strides=4, padding='same'))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Conv2D(256, 3, strides=4, padding='same'))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(4096, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(4096, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=\"Adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=5,\n",
    "    # validation_data=validation_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history.history['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = models.Sequential(\n",
    "    [\n",
    "        layers.Rescaling(1.0 / 255, input_shape=(32, 32, 3)),\n",
    "        layers.Flatten(input_shape=(32, 32, 3)),\n",
    "        layers.Dense(3000, activation=\"relu\"),\n",
    "        layers.Dense(1000, activation=\"relu\"),\n",
    "        layers.Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "ann.compile(optimizer=\"SGD\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "ann.fit(\n",
    "    train_dataset,\n",
    "    epochs=5,\n",
    "    # validation_data=validation_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets, layers, models, losses, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model152 = tf.keras.applications.ResNet152(weights = 'imagenet', include_top = False, input_shape = (32,32,3))\n",
    "for layer in base_model152.layers:\n",
    "  layer.trainable = False\n",
    "\n",
    "\n",
    "base_model101 = tf.keras.applications.ResNet101(weights = 'imagenet', include_top = False, input_shape = (32,32,3))\n",
    "for layer in base_model101.layers:\n",
    "  layer.trainable = False\n",
    "\n",
    "\n",
    "base_model50 = tf.keras.applications.ResNet50(weights = 'imagenet', include_top = False, input_shape = (32,32,3))\n",
    "for layer in base_model50.layers:\n",
    "  layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model152 = tf.keras.applications.ResNet152(weights = 'imagenet', include_top = False, input_shape = (32,32,3))\n",
    "for layer in base_model152.layers:\n",
    "  layer.trainable = False\n",
    "\n",
    "x = layers.Flatten()(base_model152.output)\n",
    "x = layers.Dense(1000, activation='relu')(x)\n",
    "predictions = layers.Dense(10, activation = 'softmax')(x)\n",
    "\n",
    "head_model = Model(inputs = base_model152.input, outputs = predictions)\n",
    "head_model.compile(optimizer=\"Adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = head_model.fit(train_dataset, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 107s 36ms/step - loss: 129.5083 - accuracy: 0.1716\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 101s 36ms/step - loss: 104.9954 - accuracy: 0.2035\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 104.8715 - accuracy: 0.2162\n",
      "50 SGD relu l1 0.1 0.21622222661972046\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 105s 37ms/step - loss: 948.7927 - accuracy: 0.1294\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 924.3524 - accuracy: 0.1364\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 924.2646 - accuracy: 0.1406\n",
      "50 SGD relu l1 0.3 0.14063332974910736\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 105s 36ms/step - loss: 2586.9675 - accuracy: 0.1303\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 2562.7554 - accuracy: 0.1336\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 101s 36ms/step - loss: 2562.6667 - accuracy: 0.1369\n",
      "50 SGD relu l1 0.5 0.13688889145851135\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 104s 36ms/step - loss: 13.7340 - accuracy: 0.4194\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 101s 36ms/step - loss: 1.7222 - accuracy: 0.4337\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 101s 36ms/step - loss: 1.7164 - accuracy: 0.4362\n",
      "50 SGD relu l2 0.1 0.43619999289512634\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 103s 36ms/step - loss: 13.8913 - accuracy: 0.3836\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 101s 36ms/step - loss: 1.8626 - accuracy: 0.3861\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 101s 36ms/step - loss: 1.8602 - accuracy: 0.3888\n",
      "50 SGD relu l2 0.3 0.38884443044662476\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 103s 36ms/step - loss: 13.9969 - accuracy: 0.3563\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 101s 36ms/step - loss: 1.9348 - accuracy: 0.3654\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 101s 36ms/step - loss: 1.9316 - accuracy: 0.3672\n",
      "50 SGD relu l2 0.5 0.3671777844429016\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 104s 36ms/step - loss: 129.2142 - accuracy: 0.1623\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 101s 36ms/step - loss: 104.6696 - accuracy: 0.1942\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 104.6625 - accuracy: 0.1984\n",
      "50 SGD linear l1 0.1 0.19840000569820404\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 104s 36ms/step - loss: 948.6490 - accuracy: 0.1346\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 101s 36ms/step - loss: 924.0101 - accuracy: 0.1377\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 924.0060 - accuracy: 0.1373\n",
      "50 SGD linear l1 0.3 0.13768889009952545\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 104s 36ms/step - loss: 2586.9055 - accuracy: 0.1358\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 2562.4365 - accuracy: 0.1386\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 2562.4338 - accuracy: 0.1370\n",
      "50 SGD linear l1 0.5 0.1386111080646515\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 104s 36ms/step - loss: 13.9005 - accuracy: 0.4198\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 101s 36ms/step - loss: 1.6962 - accuracy: 0.4309\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 101s 36ms/step - loss: 1.6956 - accuracy: 0.4300\n",
      "50 SGD linear l2 0.1 0.43086665868759155\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 104s 36ms/step - loss: 14.0998 - accuracy: 0.3821\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 1.8343 - accuracy: 0.3903\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 103s 37ms/step - loss: 1.8322 - accuracy: 0.3906\n",
      "50 SGD linear l2 0.3 0.39055556058883667\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 104s 36ms/step - loss: 14.1910 - accuracy: 0.3586\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 101s 36ms/step - loss: 1.9040 - accuracy: 0.3680\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 101s 36ms/step - loss: 1.9036 - accuracy: 0.3670\n",
      "50 SGD linear l2 0.5 0.36802223324775696\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 105s 37ms/step - loss: 128.5863 - accuracy: 0.0999\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 103s 37ms/step - loss: 104.7029 - accuracy: 0.0988\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 103s 37ms/step - loss: 104.7026 - accuracy: 0.0991\n",
      "50 SGD softmax l1 0.1 0.09992222487926483\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 105s 37ms/step - loss: 947.6859 - accuracy: 0.0997\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 104s 37ms/step - loss: 923.8989 - accuracy: 0.0997\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 104s 37ms/step - loss: 923.8933 - accuracy: 0.0984\n",
      "50 SGD softmax l1 0.3 0.09968888759613037\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 106s 37ms/step - loss: 2585.9109 - accuracy: 0.1001\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 103s 37ms/step - loss: 2562.3176 - accuracy: 0.1000\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 103s 37ms/step - loss: 2562.2688 - accuracy: 0.1005\n",
      "50 SGD softmax l1 0.5 0.10047777742147446\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 104s 36ms/step - loss: 14.1506 - accuracy: 0.1828\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 2.0715 - accuracy: 0.1938\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 2.0412 - accuracy: 0.2056\n",
      "50 SGD softmax l2 0.1 0.20564444363117218\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 104s 36ms/step - loss: 14.2921 - accuracy: 0.1020\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 2.3026 - accuracy: 0.1015\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 2.3026 - accuracy: 0.1019\n",
      "50 SGD softmax l2 0.3 0.10196666419506073\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 104s 36ms/step - loss: 14.3085 - accuracy: 0.1010\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 2.3026 - accuracy: 0.1007\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 101s 36ms/step - loss: 2.3026 - accuracy: 0.1005\n",
      "50 SGD softmax l2 0.5 0.10100000351667404\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 105s 36ms/step - loss: 52.5876 - accuracy: 0.2777\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 101s 36ms/step - loss: 24.3051 - accuracy: 0.3198\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 101s 36ms/step - loss: 24.1276 - accuracy: 0.3317\n",
      "50 Adam relu l1 0.1 0.3317444324493408\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 104s 36ms/step - loss: 151.2536 - accuracy: 0.2255\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 68.7680 - accuracy: 0.2723\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 68.7895 - accuracy: 0.2840\n",
      "50 Adam relu l1 0.3 0.2840000092983246\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 104s 36ms/step - loss: 249.4246 - accuracy: 0.1978\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 112.5725 - accuracy: 0.2423\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 112.4851 - accuracy: 0.2647\n",
      "50 Adam relu l1 0.5 0.2646999955177307\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 104s 36ms/step - loss: 3.0765 - accuracy: 0.3989\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 1.7831 - accuracy: 0.4268\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 1.7449 - accuracy: 0.4323\n",
      "50 Adam relu l2 0.1 0.4323444366455078\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 104s 36ms/step - loss: 4.2311 - accuracy: 0.3755\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 103s 36ms/step - loss: 1.8712 - accuracy: 0.4004\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 1.8365 - accuracy: 0.4106\n",
      "50 Adam relu l2 0.3 0.41056665778160095\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 104s 36ms/step - loss: 5.4275 - accuracy: 0.3578\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 101s 36ms/step - loss: 1.9347 - accuracy: 0.3849\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 101s 36ms/step - loss: 1.8880 - accuracy: 0.3965\n",
      "50 Adam relu l2 0.5 0.396533340215683\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 105s 36ms/step - loss: 54.1524 - accuracy: 0.2496\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 25.2451 - accuracy: 0.3117\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 25.2980 - accuracy: 0.3356\n",
      "50 Adam linear l1 0.1 0.33558890223503113\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 104s 36ms/step - loss: 153.5455 - accuracy: 0.1877\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 69.9031 - accuracy: 0.2400\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 69.2033 - accuracy: 0.2711\n",
      "50 Adam linear l1 0.3 0.2711222171783447\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 106s 37ms/step - loss: 252.5554 - accuracy: 0.1695\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 105s 37ms/step - loss: 114.0734 - accuracy: 0.2087\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 116s 41ms/step - loss: 112.5920 - accuracy: 0.2509\n",
      "50 Adam linear l1 0.5 0.2508888840675354\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 104s 36ms/step - loss: 4.0849 - accuracy: 0.3855\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 1.8825 - accuracy: 0.4178\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 1.9072 - accuracy: 0.4232\n",
      "50 Adam linear l2 0.1 0.42322221398353577\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 104s 36ms/step - loss: 5.2103 - accuracy: 0.3585\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 2.0004 - accuracy: 0.3985\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 2.0249 - accuracy: 0.4071\n",
      "50 Adam linear l2 0.3 0.4070777893066406\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 103s 36ms/step - loss: 6.1689 - accuracy: 0.3443\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 101s 36ms/step - loss: 2.0481 - accuracy: 0.3894\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 101s 36ms/step - loss: 2.0873 - accuracy: 0.3970\n",
      "50 Adam linear l2 0.5 0.3970000147819519\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 105s 36ms/step - loss: 53.3921 - accuracy: 0.1013\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 25.9542 - accuracy: 0.0999\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 25.6408 - accuracy: 0.1008\n",
      "50 Adam softmax l1 0.1 0.10126666724681854\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 106s 37ms/step - loss: 157.1653 - accuracy: 0.1003\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 103s 36ms/step - loss: 75.2289 - accuracy: 0.1004\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 103s 37ms/step - loss: 74.3614 - accuracy: 0.1014\n",
      "50 Adam softmax l1 0.3 0.10136666893959045\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 105s 36ms/step - loss: 260.7472 - accuracy: 0.1012\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 103s 37ms/step - loss: 125.0007 - accuracy: 0.0995\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 123.6998 - accuracy: 0.1008\n",
      "50 Adam softmax l1 0.5 0.10119999945163727\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 105s 36ms/step - loss: 2.6678 - accuracy: 0.2360\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 1.9474 - accuracy: 0.3022\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 1.8985 - accuracy: 0.3255\n",
      "50 Adam softmax l2 0.1 0.3254888951778412\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 107s 37ms/step - loss: 3.9614 - accuracy: 0.1683\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 103s 36ms/step - loss: 2.0174 - accuracy: 0.2580\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 103s 36ms/step - loss: 1.9657 - accuracy: 0.2743\n",
      "50 Adam softmax l2 0.3 0.2743111252784729\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 104s 36ms/step - loss: 5.2202 - accuracy: 0.1274\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 101s 36ms/step - loss: 2.0624 - accuracy: 0.2382\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 1.9908 - accuracy: 0.2708\n",
      "50 Adam softmax l2 0.5 0.27079999446868896\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 104s 36ms/step - loss: 218.4728 - accuracy: 0.2361\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 101s 36ms/step - loss: 3.8671 - accuracy: 0.2119\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 101s 36ms/step - loss: 3.4614 - accuracy: 0.2222\n",
      "50 Adagrad relu l1 0.1 0.23606666922569275\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 106s 37ms/step - loss: 503.0448 - accuracy: 0.1495\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 7.0714 - accuracy: 0.1341\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 103s 36ms/step - loss: 5.9391 - accuracy: 0.1343\n",
      "50 Adagrad relu l1 0.3 0.14949999749660492\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 104s 36ms/step - loss: 799.9435 - accuracy: 0.1222\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 10.1671 - accuracy: 0.1245\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 8.2430 - accuracy: 0.1238\n",
      "50 Adagrad relu l1 0.5 0.12451110780239105\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 104s 36ms/step - loss: 45.6761 - accuracy: 0.4466\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 101s 36ms/step - loss: 7.0145 - accuracy: 0.5055\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 101s 36ms/step - loss: 3.4112 - accuracy: 0.5166\n",
      "50 Adagrad relu l2 0.1 0.5165888667106628\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 104s 36ms/step - loss: 54.6180 - accuracy: 0.4451\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 2.5610 - accuracy: 0.4854\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 103s 36ms/step - loss: 1.8907 - accuracy: 0.4936\n",
      "50 Adagrad relu l2 0.3 0.4936000108718872\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 104s 36ms/step - loss: 61.1034 - accuracy: 0.4426\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 2.0257 - accuracy: 0.4766\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 101s 36ms/step - loss: 1.7164 - accuracy: 0.4815\n",
      "50 Adagrad relu l2 0.5 0.48153331875801086\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 105s 36ms/step - loss: 222.1150 - accuracy: 0.2477\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 3.8776 - accuracy: 0.2061\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 3.4720 - accuracy: 0.2117\n",
      "50 Adagrad linear l1 0.1 0.24772222340106964\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 105s 36ms/step - loss: 504.6808 - accuracy: 0.1557\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 101s 36ms/step - loss: 7.1070 - accuracy: 0.1392\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 5.9895 - accuracy: 0.1401\n",
      "50 Adagrad linear l1 0.3 0.15574444830417633\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 105s 36ms/step - loss: 799.0923 - accuracy: 0.1480\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 10.3038 - accuracy: 0.1374\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 8.4421 - accuracy: 0.1391\n",
      "50 Adagrad linear l1 0.5 0.14802221953868866\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 104s 36ms/step - loss: 49.9137 - accuracy: 0.4410\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 10.6502 - accuracy: 0.4956\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 5.5653 - accuracy: 0.5069\n",
      "50 Adagrad linear l2 0.1 0.5068666934967041\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 103s 36ms/step - loss: 60.1803 - accuracy: 0.4429\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 101s 36ms/step - loss: 4.0427 - accuracy: 0.4848\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 2.5085 - accuracy: 0.4924\n",
      "50 Adagrad linear l2 0.3 0.49237778782844543\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 103s 36ms/step - loss: 66.1134 - accuracy: 0.4373\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 100s 36ms/step - loss: 2.7543 - accuracy: 0.4757\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 101s 36ms/step - loss: 1.9775 - accuracy: 0.4827\n",
      "50 Adagrad linear l2 0.5 0.48267778754234314\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 106s 37ms/step - loss: 216.1852 - accuracy: 0.0984\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 103s 36ms/step - loss: 3.9000 - accuracy: 0.0995\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 3.5290 - accuracy: 0.0990\n",
      "50 Adagrad softmax l1 0.1 0.0995333343744278\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 105s 37ms/step - loss: 503.0930 - accuracy: 0.0944\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 7.0992 - accuracy: 0.0957\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 103s 36ms/step - loss: 5.9838 - accuracy: 0.0967\n",
      "50 Adagrad softmax l1 0.3 0.09665555506944656\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 105s 37ms/step - loss: 797.8335 - accuracy: 0.0970\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 103s 36ms/step - loss: 10.2962 - accuracy: 0.0923\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 8.4377 - accuracy: 0.0901\n",
      "50 Adagrad softmax l1 0.5 0.09702222049236298\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 105s 36ms/step - loss: 41.7001 - accuracy: 0.1524\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 103s 36ms/step - loss: 4.1050 - accuracy: 0.2132\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 2.3441 - accuracy: 0.2222\n",
      "50 Adagrad softmax l2 0.1 0.22215555608272552\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 104s 36ms/step - loss: 50.6916 - accuracy: 0.1093\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 2.3663 - accuracy: 0.1403\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 2.3027 - accuracy: 0.1436\n",
      "50 Adagrad softmax l2 0.3 0.14356666803359985\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 104s 36ms/step - loss: 57.8297 - accuracy: 0.1130\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 101s 36ms/step - loss: 2.3148 - accuracy: 0.1059\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 102s 36ms/step - loss: 2.3026 - accuracy: 0.1046\n",
      "50 Adagrad softmax l2 0.5 0.11295555531978607\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 157s 54ms/step - loss: 129.7135 - accuracy: 0.1355\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 154s 55ms/step - loss: 105.1398 - accuracy: 0.1392\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 156s 55ms/step - loss: 105.0337 - accuracy: 0.1453\n",
      "101 SGD relu l1 0.1 0.14527778327465057\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 160s 55ms/step - loss: 948.9196 - accuracy: 0.1291\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 156s 55ms/step - loss: 924.3320 - accuracy: 0.1334\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 156s 55ms/step - loss: 924.2416 - accuracy: 0.1377\n",
      "101 SGD relu l1 0.3 0.1377333402633667\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 162s 56ms/step - loss: 2587.1113 - accuracy: 0.1259\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 156s 55ms/step - loss: 2562.7346 - accuracy: 0.1323\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 156s 55ms/step - loss: 2562.6497 - accuracy: 0.1364\n",
      "101 SGD relu l1 0.5 0.13635554909706116\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 160s 55ms/step - loss: 13.7709 - accuracy: 0.4094\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 155s 55ms/step - loss: 1.7595 - accuracy: 0.4226\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 156s 55ms/step - loss: 1.7555 - accuracy: 0.4233\n",
      "101 SGD relu l2 0.1 0.42332223057746887\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 160s 55ms/step - loss: 13.9350 - accuracy: 0.3723\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 156s 55ms/step - loss: 1.8965 - accuracy: 0.3807\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 156s 55ms/step - loss: 1.8940 - accuracy: 0.3800\n",
      "101 SGD relu l2 0.3 0.3806777894496918\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 159s 55ms/step - loss: 14.0206 - accuracy: 0.3504\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 155s 55ms/step - loss: 1.9680 - accuracy: 0.3574\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 154s 55ms/step - loss: 1.9655 - accuracy: 0.3571\n",
      "101 SGD relu l2 0.5 0.35742223262786865\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 160s 55ms/step - loss: 129.2989 - accuracy: 0.1345\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 156s 55ms/step - loss: 104.7867 - accuracy: 0.1366\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 156s 56ms/step - loss: 104.7548 - accuracy: 0.1666\n",
      "101 SGD linear l1 0.1 0.16655555367469788\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 160s 55ms/step - loss: 948.7344 - accuracy: 0.1325\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 156s 55ms/step - loss: 924.0150 - accuracy: 0.1366\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 155s 55ms/step - loss: 924.0148 - accuracy: 0.1385\n",
      "101 SGD linear l1 0.3 0.13846667110919952\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 160s 55ms/step - loss: 2587.0603 - accuracy: 0.1346\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 156s 55ms/step - loss: 2562.4587 - accuracy: 0.1383\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 156s 55ms/step - loss: 2562.4490 - accuracy: 0.1374\n",
      "101 SGD linear l1 0.5 0.13830000162124634\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 161s 56ms/step - loss: 13.9578 - accuracy: 0.4090\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 156s 55ms/step - loss: 1.7284 - accuracy: 0.4189\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 158s 56ms/step - loss: 1.7299 - accuracy: 0.4205\n",
      "101 SGD linear l2 0.1 0.42054444551467896\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 160s 55ms/step - loss: 14.1322 - accuracy: 0.3753\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 156s 55ms/step - loss: 1.8585 - accuracy: 0.3806\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 155s 55ms/step - loss: 1.8633 - accuracy: 0.3812\n",
      "101 SGD linear l2 0.3 0.3812222182750702\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 160s 55ms/step - loss: 14.2352 - accuracy: 0.3520\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 155s 55ms/step - loss: 1.9320 - accuracy: 0.3585\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 155s 55ms/step - loss: 1.9311 - accuracy: 0.3586\n",
      "101 SGD linear l2 0.5 0.3586222231388092\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 161s 56ms/step - loss: 128.5733 - accuracy: 0.0999\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 157s 56ms/step - loss: 104.7028 - accuracy: 0.0987\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 157s 56ms/step - loss: 104.7025 - accuracy: 0.0992\n",
      "101 SGD softmax l1 0.1 0.09991110861301422\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 161s 56ms/step - loss: 947.6830 - accuracy: 0.0996\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 157s 56ms/step - loss: 923.9006 - accuracy: 0.0984\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 157s 56ms/step - loss: 923.8979 - accuracy: 0.0997\n",
      "101 SGD softmax l1 0.3 0.09968888759613037\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 162s 56ms/step - loss: 2585.8853 - accuracy: 0.1005\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 158s 56ms/step - loss: 2562.2954 - accuracy: 0.1004\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 158s 56ms/step - loss: 2562.2915 - accuracy: 0.1013\n",
      "101 SGD softmax l1 0.5 0.10133333504199982\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 161s 56ms/step - loss: 14.2055 - accuracy: 0.1916\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 157s 56ms/step - loss: 2.1110 - accuracy: 0.2378\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 157s 56ms/step - loss: 2.0532 - accuracy: 0.2461\n",
      "101 SGD softmax l2 0.1 0.24611110985279083\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 162s 56ms/step - loss: 14.2685 - accuracy: 0.1009\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 157s 56ms/step - loss: 2.3026 - accuracy: 0.1022\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 157s 56ms/step - loss: 2.3026 - accuracy: 0.1017\n",
      "101 SGD softmax l2 0.3 0.10220000147819519\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 160s 55ms/step - loss: 14.3081 - accuracy: 0.1009\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 158s 56ms/step - loss: 2.3026 - accuracy: 0.1001\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 156s 56ms/step - loss: 2.3026 - accuracy: 0.1008\n",
      "101 SGD softmax l2 0.5 0.10086666792631149\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 162s 56ms/step - loss: 53.0207 - accuracy: 0.2791\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 158s 56ms/step - loss: 24.4156 - accuracy: 0.3205\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 158s 56ms/step - loss: 24.2163 - accuracy: 0.3247\n",
      "101 Adam relu l1 0.1 0.3247222304344177\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 162s 56ms/step - loss: 151.9417 - accuracy: 0.2190\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 157s 56ms/step - loss: 68.8583 - accuracy: 0.2592\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 157s 56ms/step - loss: 68.4997 - accuracy: 0.2628\n",
      "101 Adam relu l1 0.3 0.2628111243247986\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 161s 56ms/step - loss: 250.0190 - accuracy: 0.1879\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 157s 56ms/step - loss: 112.7116 - accuracy: 0.2516\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 157s 56ms/step - loss: 112.5555 - accuracy: 0.2567\n",
      "101 Adam relu l1 0.5 0.25671112537384033\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 163s 56ms/step - loss: 3.2231 - accuracy: 0.3909\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 158s 56ms/step - loss: 1.8121 - accuracy: 0.4144\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 158s 56ms/step - loss: 1.7774 - accuracy: 0.4198\n",
      "101 Adam relu l2 0.1 0.41984444856643677\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 160s 55ms/step - loss: 4.4238 - accuracy: 0.3637\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 157s 56ms/step - loss: 1.9207 - accuracy: 0.3878\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 156s 56ms/step - loss: 1.8753 - accuracy: 0.3952\n",
      "101 Adam relu l2 0.3 0.3952111005783081\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 162s 56ms/step - loss: 5.5543 - accuracy: 0.3506\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 157s 56ms/step - loss: 1.9762 - accuracy: 0.3751\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 157s 56ms/step - loss: 1.9332 - accuracy: 0.3844\n",
      "101 Adam relu l2 0.5 0.3844222128391266\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 161s 55ms/step - loss: 54.6595 - accuracy: 0.2602\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 157s 56ms/step - loss: 25.4996 - accuracy: 0.3206\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 158s 56ms/step - loss: 25.6501 - accuracy: 0.3294\n",
      "101 Adam linear l1 0.1 0.3294222354888916\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 161s 55ms/step - loss: 154.9553 - accuracy: 0.1779\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 156s 55ms/step - loss: 70.6163 - accuracy: 0.2419\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 157s 56ms/step - loss: 69.7604 - accuracy: 0.2636\n",
      "101 Adam linear l1 0.3 0.2635555565357208\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 163s 56ms/step - loss: 253.8502 - accuracy: 0.1542\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 157s 56ms/step - loss: 114.4986 - accuracy: 0.2440\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 157s 56ms/step - loss: 113.0663 - accuracy: 0.2538\n",
      "101 Adam linear l1 0.5 0.25376665592193604\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 160s 55ms/step - loss: 4.5379 - accuracy: 0.3767\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 155s 55ms/step - loss: 1.9515 - accuracy: 0.4044\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 157s 56ms/step - loss: 1.9735 - accuracy: 0.4116\n",
      "101 Adam linear l2 0.1 0.41155555844306946\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 162s 56ms/step - loss: 5.8032 - accuracy: 0.3498\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 157s 56ms/step - loss: 2.0698 - accuracy: 0.3794\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 157s 56ms/step - loss: 2.0818 - accuracy: 0.3908\n",
      "101 Adam linear l2 0.3 0.3908444344997406\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 160s 55ms/step - loss: 6.6271 - accuracy: 0.3381\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 156s 55ms/step - loss: 2.1034 - accuracy: 0.3757\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 156s 55ms/step - loss: 2.1335 - accuracy: 0.3847\n",
      "101 Adam linear l2 0.5 0.3847000002861023\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 165s 57ms/step - loss: 53.3473 - accuracy: 0.1001\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 160s 57ms/step - loss: 25.8377 - accuracy: 0.0999\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 159s 56ms/step - loss: 25.5244 - accuracy: 0.1004\n",
      "101 Adam softmax l1 0.1 0.10044444352388382\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 162s 56ms/step - loss: 156.8493 - accuracy: 0.1010\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 159s 56ms/step - loss: 74.9045 - accuracy: 0.1010\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 155s 55ms/step - loss: 73.9651 - accuracy: 0.1000\n",
      "101 Adam softmax l1 0.3 0.10102222114801407\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 171s 59ms/step - loss: 260.6558 - accuracy: 0.1008\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 168s 60ms/step - loss: 124.7139 - accuracy: 0.1005\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 165s 59ms/step - loss: 123.3211 - accuracy: 0.1008\n",
      "101 Adam softmax l1 0.5 0.10081110894680023\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 157s 54ms/step - loss: 2.6595 - accuracy: 0.2694\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 153s 55ms/step - loss: 1.9273 - accuracy: 0.3201\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 154s 55ms/step - loss: 1.8912 - accuracy: 0.3358\n",
      "101 Adam softmax l2 0.1 0.3357999920845032\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 159s 55ms/step - loss: 3.9092 - accuracy: 0.2187\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 155s 55ms/step - loss: 2.0278 - accuracy: 0.2582\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 154s 55ms/step - loss: 2.0012 - accuracy: 0.2655\n",
      "101 Adam softmax l2 0.3 0.26552221179008484\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 156s 54ms/step - loss: 5.2225 - accuracy: 0.1302\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 153s 54ms/step - loss: 2.0752 - accuracy: 0.2457\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 153s 54ms/step - loss: 2.0253 - accuracy: 0.2635\n",
      "101 Adam softmax l2 0.5 0.2635444402694702\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 160s 55ms/step - loss: 218.9920 - accuracy: 0.2260\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 155s 55ms/step - loss: 3.8932 - accuracy: 0.2225\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 154s 55ms/step - loss: 3.4874 - accuracy: 0.2316\n",
      "101 Adagrad relu l1 0.1 0.23162221908569336\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 158s 54ms/step - loss: 504.2401 - accuracy: 0.1465\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 154s 55ms/step - loss: 7.0940 - accuracy: 0.1450\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 154s 55ms/step - loss: 5.9676 - accuracy: 0.1436\n",
      "101 Adagrad relu l1 0.3 0.1464666724205017\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 161s 55ms/step - loss: 798.3274 - accuracy: 0.1216\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 156s 55ms/step - loss: 10.2229 - accuracy: 0.1283\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 155s 55ms/step - loss: 8.3157 - accuracy: 0.1299\n",
      "101 Adagrad relu l1 0.5 0.1298888921737671\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 158s 55ms/step - loss: 46.7988 - accuracy: 0.4310\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 154s 55ms/step - loss: 7.7969 - accuracy: 0.4951\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 154s 55ms/step - loss: 3.7839 - accuracy: 0.5090\n",
      "101 Adagrad relu l2 0.1 0.5090222358703613\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 162s 56ms/step - loss: 56.2629 - accuracy: 0.4321\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 152s 54ms/step - loss: 2.8675 - accuracy: 0.4777\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 151s 54ms/step - loss: 2.0014 - accuracy: 0.4873\n",
      "101 Adagrad relu l2 0.3 0.4872555434703827\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 154s 53ms/step - loss: 61.8927 - accuracy: 0.4302\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 149s 53ms/step - loss: 2.0802 - accuracy: 0.4662\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 149s 53ms/step - loss: 1.7496 - accuracy: 0.4728\n",
      "101 Adagrad relu l2 0.5 0.47281110286712646\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 157s 54ms/step - loss: 224.2767 - accuracy: 0.2385\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 152s 54ms/step - loss: 3.9192 - accuracy: 0.2178\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 152s 54ms/step - loss: 3.5010 - accuracy: 0.2322\n",
      "101 Adagrad linear l1 0.1 0.23845554888248444\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 155s 54ms/step - loss: 505.9502 - accuracy: 0.1508\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 149s 53ms/step - loss: 7.1086 - accuracy: 0.1418\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 151s 54ms/step - loss: 5.9906 - accuracy: 0.1412\n",
      "101 Adagrad linear l1 0.3 0.15077777206897736\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 157s 54ms/step - loss: 799.4202 - accuracy: 0.1354\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 152s 54ms/step - loss: 10.3046 - accuracy: 0.1302\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 152s 54ms/step - loss: 8.4430 - accuracy: 0.1338\n",
      "101 Adagrad linear l1 0.5 0.13544444739818573\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 154s 53ms/step - loss: 52.4798 - accuracy: 0.4258\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 149s 53ms/step - loss: 12.9227 - accuracy: 0.4858\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 150s 53ms/step - loss: 6.9321 - accuracy: 0.4985\n",
      "101 Adagrad linear l2 0.1 0.49845555424690247\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 156s 54ms/step - loss: 63.7217 - accuracy: 0.4290\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 151s 54ms/step - loss: 4.9692 - accuracy: 0.4745\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 151s 54ms/step - loss: 2.8553 - accuracy: 0.4811\n",
      "101 Adagrad linear l2 0.3 0.4810999929904938\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 155s 53ms/step - loss: 68.5062 - accuracy: 0.4266\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 150s 53ms/step - loss: 3.0313 - accuracy: 0.4654\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 150s 53ms/step - loss: 2.0804 - accuracy: 0.4720\n",
      "101 Adagrad linear l2 0.5 0.47202223539352417\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 161s 55ms/step - loss: 215.6355 - accuracy: 0.1001\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 154s 55ms/step - loss: 3.8994 - accuracy: 0.1000\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 156s 55ms/step - loss: 3.5287 - accuracy: 0.1000\n",
      "101 Adagrad softmax l1 0.1 0.10012222081422806\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 159s 55ms/step - loss: 503.0645 - accuracy: 0.0980\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 156s 55ms/step - loss: 7.0980 - accuracy: 0.1000\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 155s 55ms/step - loss: 5.9820 - accuracy: 0.1000\n",
      "101 Adagrad softmax l1 0.3 0.10000000149011612\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 161s 55ms/step - loss: 798.2847 - accuracy: 0.0986\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 156s 55ms/step - loss: 10.2953 - accuracy: 0.1000\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 156s 55ms/step - loss: 8.4349 - accuracy: 0.1000\n",
      "101 Adagrad softmax l1 0.5 0.10000000149011612\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 160s 55ms/step - loss: 41.6903 - accuracy: 0.1581\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 156s 55ms/step - loss: 4.1046 - accuracy: 0.1748\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 155s 55ms/step - loss: 2.3470 - accuracy: 0.1765\n",
      "101 Adagrad softmax l2 0.1 0.17652222514152527\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 160s 55ms/step - loss: 50.6915 - accuracy: 0.1408\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 155s 55ms/step - loss: 2.3575 - accuracy: 0.1543\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 156s 55ms/step - loss: 2.2899 - accuracy: 0.1632\n",
      "101 Adagrad softmax l2 0.3 0.16317777335643768\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 158s 54ms/step - loss: 57.9360 - accuracy: 0.1313\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 155s 55ms/step - loss: 2.3148 - accuracy: 0.1439\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 155s 55ms/step - loss: 2.3026 - accuracy: 0.1445\n",
      "101 Adagrad softmax l2 0.5 0.14446666836738586\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 211s 73ms/step - loss: 129.7965 - accuracy: 0.1397\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 206s 73ms/step - loss: 105.1349 - accuracy: 0.1443\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 207s 73ms/step - loss: 105.0185 - accuracy: 0.1702\n",
      "152 SGD relu l1 0.1 0.17016667127609253\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 215s 74ms/step - loss: 948.9807 - accuracy: 0.1335\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 216s 77ms/step - loss: 924.3286 - accuracy: 0.1438\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 227s 81ms/step - loss: 924.2470 - accuracy: 0.1476\n",
      "152 SGD relu l1 0.3 0.14758889377117157\n",
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 233s 80ms/step - loss: 2587.1733 - accuracy: 0.1301\n",
      "Epoch 2/3\n",
      "1445/2813 [==============>...............] - ETA: 1:48 - loss: 2562.7612 - accuracy: 0.1374"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m head_model \u001b[38;5;241m=\u001b[39m Model(inputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39minput, outputs \u001b[38;5;241m=\u001b[39m predictions)\n\u001b[1;32m     13\u001b[0m head_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimize_algo, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 15\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mhead_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(model_nr, optimize_algo, activation_func, regularization_algo_name, l, \u001b[38;5;28mmax\u001b[39m(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     17\u001b[0m file\u001b[38;5;241m.\u001b[39mwrite(model_nr \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m optimize_algo \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m activation_func \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m regularization_algo_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(l) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mmax\u001b[39m(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1651\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open('results.txt', 'a') as file:\n",
    "    for model, model_nr in zip([base_model50, base_model101, base_model152], ['50', '101', '152']):\n",
    "        for optimize_algo in ['SGD', 'Adam', 'Adagrad']:\n",
    "            for activation_func in ['relu', 'linear', 'softmax']:\n",
    "                for regularization_algo, regularization_algo_name in zip([tf.keras.regularizers.L1, tf.keras.regularizers.L2], ['l1', 'l2']):\n",
    "                    for l in [0.1, 0.3, 0.5]:\n",
    "\n",
    "                        x = layers.Flatten()(model.output)\n",
    "                        x = layers.Dense(1000, activation=activation_func, kernel_regularizer=regularization_algo(l))(x)\n",
    "                        predictions = layers.Dense(10, activation = 'softmax')(x)\n",
    "\n",
    "                        head_model = Model(inputs = model.input, outputs = predictions)\n",
    "                        head_model.compile(optimizer=optimize_algo, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "                        history = head_model.fit(train_dataset, epochs=3)\n",
    "                        print(model_nr, optimize_algo, activation_func, regularization_algo_name, l, max(history.history['accuracy']))\n",
    "                        file.write(model_nr + ' ' + optimize_algo + ' ' + activation_func + ' ' + regularization_algo_name + ' ' + str(l) + ' ' + str(max(history.history['accuracy'])) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('results.txt', sep=' ', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>50</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>relu</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.516589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>101</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>relu</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.509022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>50</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>linear</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.506867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>101</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>linear</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.498456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>50</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>relu</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.493600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>50</td>\n",
       "      <td>SGD</td>\n",
       "      <td>softmax</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.099689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>101</td>\n",
       "      <td>SGD</td>\n",
       "      <td>softmax</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.099689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>50</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>softmax</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.099533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>50</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>softmax</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.097022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>softmax</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.096656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0        1        2   3    4         5\n",
       "39   50  Adagrad     relu  l2  0.1  0.516589\n",
       "93  101  Adagrad     relu  l2  0.1  0.509022\n",
       "45   50  Adagrad   linear  l2  0.1  0.506867\n",
       "99  101  Adagrad   linear  l2  0.1  0.498456\n",
       "40   50  Adagrad     relu  l2  0.3  0.493600\n",
       "..  ...      ...      ...  ..  ...       ...\n",
       "13   50      SGD  softmax  l1  0.3  0.099689\n",
       "67  101      SGD  softmax  l1  0.3  0.099689\n",
       "48   50  Adagrad  softmax  l1  0.1  0.099533\n",
       "50   50  Adagrad  softmax  l1  0.5  0.097022\n",
       "49   50  Adagrad  softmax  l1  0.3  0.096656\n",
       "\n",
       "[110 rows x 6 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sort_values(5, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "tf.Tensor(\n",
      "[[22. 28.]\n",
      " [49. 64.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# Create some tensors\n",
    "a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "c = tf.matmul(a, b)\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.debugging.set_log_device_placement(True)\n",
    "tf.ones([])\n",
    "# [...] op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
    "with tf.device(\"CPU\"):\n",
    " tf.ones([])\n",
    "# [...] op Fill in device /job:localhost/replica:0/task:0/device:CPU:0\n",
    "tf.debugging.set_log_device_placement(False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
