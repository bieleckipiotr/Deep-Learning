{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import image_dataset_from_directory\n",
    "from tensorflow import clip_by_value\n",
    "from tensorflow import data as tf_data\n",
    "from tensorflow import image as tf_image\n",
    "from tensorflow import random as tf_random\n",
    "import keras_cv\n",
    "from keras import layers, models\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.regularizers import l1_l2\n",
    "input_shape = (32, 32, 3)\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Input, Rescaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = image_dataset_from_directory(\n",
    "    '../data/cinic-10_image_classification_challenge-dataset/train/',\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed = 420,\n",
    "    image_size=(32,32),\n",
    "    batch_size=32,\n",
    "    label_mode = 'categorical')\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "\n",
    "val_ds = image_dataset_from_directory(\n",
    "    '../data/cinic-10_image_classification_challenge-dataset/train/',\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed = 420,\n",
    "    image_size=(32,32),\n",
    "    batch_size=32,\n",
    "    label_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_and_preprocess_image(image_path, label):\n",
    "#     # Load image\n",
    "#     image = tf.io.read_file(image_path)\n",
    "#     # Decode PNG image to tensor\n",
    "#     image = tf.image.decode_png(image, channels=3)  # Adjust channels according to your images\n",
    "#     # Normalize pixel values to range [0, 1]\n",
    "#     image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "#     return image, label\n",
    "\n",
    "\n",
    "# def preprocess_data():\n",
    "\n",
    "#     class_names = os.listdir(data_dir)\n",
    "\n",
    "#     image_paths = []\n",
    "#     labels = []\n",
    "#     for class_name in class_names:\n",
    "#         class_dir = os.path.join(data_dir, class_name)\n",
    "#         for image_name in os.listdir(class_dir):\n",
    "#             image_path = os.path.join(class_dir, image_name)\n",
    "#             image_paths.append(image_path)\n",
    "#             labels.append(class_names.index(class_name))\n",
    "\n",
    "#     # Create TensorFlow Dataset from the loaded data\n",
    "#     dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "#     dataset = dataset.map(load_and_preprocess_image)\n",
    "\n",
    "#     dataset = dataset.shuffle(buffer_size=10000)\n",
    "\n",
    "# # Split the dataset into train, validation, and test sets\n",
    "#     train_size = int(0.8 * len(dataset))\n",
    "#     test_size = int(0.2 * len(dataset))\n",
    "\n",
    "#     train_dataset = dataset.take(train_size)\n",
    "#     test_dataset = dataset.skip(train_size).take(test_size)\n",
    "#     train_dataset = train_dataset.shuffle(buffer_size=len(image_paths)).batch(32)\n",
    "#     test_dataset = test_dataset.shuffle(buffer_size=len(image_paths)).batch(32)\n",
    "#     return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre-made dataset from cifar dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(input_shape, l1=0.01, l2=0.01, dropout_rate=0.2):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Convolutional layers\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape, kernel_regularizer=l1_l2(l1, l2))) # output shape is 30x30x32\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D((2, 2))) # output shape is 15x15x32\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l1_l2(l1, l2))) # output shape is 13x13x64\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D((2, 2))) # output shape is 6x6x64\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "\n",
    "    # model.add(layers.Conv2D(128, (3, 3), activation='relu', kernel_regularizer=l1_l2(l1, l2)))\n",
    "    # model.add(layers.BatchNormalization())\n",
    "    # model.add(layers.MaxPooling2D((2,2)))\n",
    "    # model.add(layers.BatchNormalization())\n",
    "    # model.add(layers.Dropout(dropout_rate))\n",
    "    # Dense layers\n",
    "    model.add(layers.Flatten()) # 1024\n",
    "    model.add(layers.Dense(256, activation='relu', kernel_regularizer=l1_l2(l1, l2)))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.Dense(10, activation='softmax', kernel_regularizer=l1_l2(l1, l2)))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cnn(input_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### searching for optimal dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = {}\n",
    "for dropout_rate in np.linspace(0, 1/2, 10):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)    \n",
    "    model = cnn(input_shape, True, dropout_rate=dropout_rate)\n",
    "    model.compile(optimizer=optimizer,\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "    print(f'dropout = {dropout_rate}')\n",
    "    history = model.fit(train_ds, validation_data = val_ds, epochs = 5)    \n",
    "    histories[dropout_rate] = {'history': history.history}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dropout in histories:\n",
    "    plt.scatter(dropout, histories[dropout]['history']['val_accuracy'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_range = np.linspace(0, 1, 10)  # Example: 10 values between 0 and 1\n",
    "l2_range = np.linspace(0, 1, 10)  # Example: 10 values between 0 and 1\n",
    "\n",
    "# Generate candidate combinations using grid search\n",
    "search_space = [(l1, l2) for l1 in l1_range for l2 in l2_range]\n",
    "for l1, l2 in search_space:\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)    \n",
    "    model = cnn(input_shape, True, l1, l2)\n",
    "    model.compile(optimizer=optimizer,\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "    print(f'l1 = {l1}, l2 = {l2}')\n",
    "    history = model.fit(train_dataset, epochs = 5)    \n",
    "    test_loss, test_acc = model.evaluate(test_dataset)\n",
    "    histories[(l1, l2)] = {'history': history.history,\n",
    "                        'test_loss': test_loss,\n",
    "                        'test_acc': test_acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (l1, l2) in histories:\n",
    "    plt.scatter(l1, l2, s = histories[(l1, l2)]['test_acc']*400, c = histories[(l1, l2)]['test_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(dropout_rate = 0.2, regul_alg=tf.keras.regularizers.L1, regul_par=0, optimizer = 'Adagrad'):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input((32,32,3)))\n",
    "    model.add(layers.Rescaling(1./255))\n",
    "\n",
    "    model.add(layers.Conv2D(64,(4,4),activation='relu', kernel_regularizer=regul_alg(regul_par)))\n",
    "    # if batch_normalization:\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    model.add(layers.Conv2D(128,(4,4),activation='relu', kernel_regularizer=regul_alg(regul_par)))\n",
    "    # if batch_normalization:\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1024,activation='relu', kernel_regularizer=regul_alg(regul_par)))\n",
    "    # if batch_normalization:\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(1024,activation='relu', kernel_regularizer=regul_alg(regul_par)))\n",
    "    # if batch_normalization:\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(10, activation = 'softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  base model, no dropout, no batch normalization after ReLu layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cnn(0)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# base model without any augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histories = {}\n",
    "for optimizer in ['SGD', 'Adagrad', 'RMSprop']:\n",
    "    for dropout in [0.1, 0.2, 0.3]:\n",
    "        for regularization_algo, regularization_algo_name in zip([tf.keras.regularizers.L1, tf.keras.regularizers.L2], ['l1', 'l2']):\n",
    "            for l in [0.001, 0.01, 0.1]:\n",
    "                if (optimizer, dropout, regularization_algo, l) in histories.keys():\n",
    "                    continue\n",
    "                print(f\"optimizer = {optimizer}, dropout = {dropout}, 'regularization_algo = {regularization_algo}, l = {l}\")\n",
    "                model = cnn(dropout, regularization_algo, l, optimizer)\n",
    "                history = model.fit(train_ds, validation_data = val_ds, epochs = 3)\n",
    "                histories[(optimizer, dropout, regularization_algo, l)] = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'optimizer' : [], 'regularizer' : [], 'lambda' : [], 'dropout' : [], 'best_acc' : []}\n",
    "for key, val in histories.items():\n",
    "    data['optimizer'].append(key[0])\n",
    "    data['regularizer'].append(key[1])\n",
    "    data['lambda'].append(key[2])\n",
    "    data['dropout'].append(key[3])\n",
    "    data['best_acc'].append(val['accuracy'][-1] if type(val) == dict else val.history['accuracy'][-1])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data).sort_values('best_acc', ascending = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
