{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### https://pytorch.org/audio/stable/tutorials/speech_recognition_pipeline_tutorial.html\n",
    "### https://colab.research.google.com/github/m3hrdadfi/soxan/blob/main/notebooks/Eating_Sound_Collection_using_Wav2Vec2.ipynb#scrollTo=Fv62ShDsH5DZ classification with pretrained transformer as base\n",
    "### https://arxiv.org/abs/2006.11477 paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device, torchaudio.list_audio_backends())\n",
    "TRAIN_DIR = './data/train/binary_classification/yes_no/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data to dataset:\n",
    "class SoundDataset(Dataset):\n",
    "\n",
    "    def __init__(self, directory, gpu):\n",
    "        self.directory = directory\n",
    "        self.classes = os.listdir(directory)\n",
    "        self.gpu = gpu\n",
    "        self.class_to_num = {cl : i for i, cl in enumerate(self.classes)}\n",
    "        self.num_to_class = {i : cl for i, cl in enumerate(self.classes)}\n",
    "        paths = []\n",
    "        for cl in self.classes:\n",
    "            tmp = [os.path.join(directory+cl, path) for path in os.listdir(directory + cl)]\n",
    "            paths+=tmp\n",
    "        self.paths = paths\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        audio_sample_path = self.paths[index]\n",
    "        label = self.paths[index].split('/')[-1].split('\\\\')[0]\n",
    "\n",
    "        signal, sr = torchaudio.load(audio_sample_path, format = 'wav')\n",
    "        signal = signal[0]\n",
    "        if self.gpu:\n",
    "            signal.to(device)\n",
    "        \n",
    "        label_numeric = self.class_to_num[label]\n",
    "        label_tensor = torch.tensor(label_numeric)\n",
    "        return signal, label_tensor\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SoundDataset(TRAIN_DIR, True)\n",
    "train_dataset, validation_dataset = random_split(dataset, [0.8, 0.2])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = 64, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size = 64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle = torchaudio.pipelines.WAV2VEC2_BASE #WAV2VEC2_ASR_BASE_960H \n",
    "model = bundle.get_model().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform = dataset[0][0]\n",
    "waveform = waveform.reshape(1, -1).to(device)\n",
    "model(waveform)[0].cpu().detach().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = model.extract_features(waveform)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[-1].cpu().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(12, 4))\n",
    "for i, ax in zip([0, 5, 11], axes):\n",
    "    feats = features[i]\n",
    "    ax.imshow(feats.detach()[0].cpu(), interpolation=\"nearest\")\n",
    "axes[0].set_title(f\"features from transformer layers 1, 6, 12\", fontsize = 15)\n",
    "fig.tight_layout()\n",
    "plt.savefig('./media/features_encoder.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wav2Vec2ClassificationModel(nn.Module):\n",
    "    def __init__(self, num_labels, hidden_size, final_dropout = 0.1, pooling = 'mean'):\n",
    "        super().__init__()\n",
    "        self.base_model = torchaudio.pipelines.WAV2VEC2_BASE.get_model().to(device)\n",
    "        self.classification_head = Wav2Vec2ClassificationHead(num_labels, hidden_size, final_dropout)\n",
    "        self.pooling = pooling\n",
    "        self.loss = []\n",
    "        self.val_accuracy = []\n",
    "        for param in self.base_model.feature_extractor.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Get features from the base model\n",
    "        features = self.base_model(inputs)[0] # 0 because it returns a tuple, we need only the first of the tuple\n",
    "\n",
    "        logits = self.classification_head(features)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        outputs = self.forward(inputs.to(device))\n",
    "        if self.pooling == 'mean':\n",
    "            predicted = torch.max(torch.mean(outputs, 1),1)[1]\n",
    "        elif self.pooling == 'max':\n",
    "            predicted = torch.max(torch.max(outputs, 1)[0],1)[1]\n",
    "        elif self.pooling == 'sum':\n",
    "            predicted = torch.max(torch.sum(outputs, 1),1)[1]\n",
    "        return predicted\n",
    "\n",
    "    \n",
    "\n",
    "    def train_model(self, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "        for epoch in range(num_epochs):\n",
    "            self.train()\n",
    "            for i, (batch_inputs, batch_labels) in enumerate(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(batch_inputs.to(device))\n",
    "                batch_labels = batch_labels.to(device)\n",
    "                if self.pooling == 'mean':\n",
    "                    outputs = torch.mean(outputs, 1)\n",
    "                elif self.pooling == 'max':\n",
    "                    outputs = torch.max(outputs, 1)[0]\n",
    "                elif self.pooling == 'sum':\n",
    "                    outputs = torch.sum(outputs, 1)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                print(f'batch number {i+1}/{len(train_loader)}, loss = { np.round(loss.item(), 4)}', end = '\\r')\n",
    "\n",
    "            # Validation\n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                total_correct = 0\n",
    "                total_samples = 0\n",
    "                for val_batch_inputs, val_batch_labels in val_loader:\n",
    "                    val_outputs = self(val_batch_inputs.to(device))\n",
    "                    \n",
    "                    if self.pooling == 'mean':\n",
    "                        predicted = torch.max(torch.mean(val_outputs, 1),1)[1]\n",
    "                    elif self.pooling == 'max':\n",
    "                        predicted = torch.max(torch.max(val_outputs, 1)[0],1)[1]\n",
    "                    elif self.pooling == 'sum':\n",
    "                        predicted = torch.max(torch.sum(val_outputs, 1),1)[1]\n",
    "                    total_correct += (predicted == val_batch_labels.to(device)).sum().item()\n",
    "                    total_samples += val_batch_labels.size(0)\n",
    "                accuracy = total_correct / total_samples\n",
    "                self.val_accuracy.append(accuracy)\n",
    "\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}, Validation Accuracy: {accuracy}')\n",
    "\n",
    "\n",
    "class Wav2Vec2ClassificationHead(nn.Module):\n",
    "    \"\"\"Head for wav2vec classification task.\"\"\"\n",
    "\n",
    "    def __init__(self, num_labels, hidden_size, final_dropout):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(final_dropout)\n",
    "        self.out_proj = nn.Linear(hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = features\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification - yes/no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SoundDataset(TRAIN_DIR, True)\n",
    "train_dataset, validation_dataset = random_split(dataset, [0.8, 0.2])\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(validation_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mean = Wav2Vec2ClassificationModel(num_labels=2, hidden_size=768, final_dropout=0.1, pooling = 'mean').to(device)\n",
    "model_max = Wav2Vec2ClassificationModel(num_labels=2, hidden_size=768, final_dropout=0.1, pooling = 'max').to(device)\n",
    "model_sum = Wav2Vec2ClassificationModel(num_labels=2, hidden_size=768, final_dropout=0.1, pooling = 'sum').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### learning rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [model_mean, model_max, model_sum]:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
    "    print(model.pooling)\n",
    "    model.train_model(train_loader, val_loader, criterion, optimizer, num_epochs=10)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(model, dataloader, classes, ax = None): # takes some time, because it requires for the model to pass through all the samples from the dataloader.\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    for batch, labels in dataloader:\n",
    "        predicted_labels+=list(model.predict(batch).cpu().numpy())\n",
    "        true_labels+=list(labels.numpy())\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                            display_labels=classes).plot(ax = ax, colorbar = False)\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,3,figsize = (12, 8))\n",
    "for ax, model in zip(axes, [model_max, model_mean, model_sum]):\n",
    "    plot_confusion_matrix(model, validation_dataloader, dataset.classes, ax = ax)\n",
    "    ax.set_title(model.pooling)\n",
    "    ax.set_ylabel('')\n",
    "ax.set_ylabel('True label')\n",
    "plt.savefig('./media/wav2vec2lr1e4.png', dpi = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### learning rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mean = Wav2Vec2ClassificationModel(num_labels=2, hidden_size=768, final_dropout=0.1, pooling = 'mean').to(device)\n",
    "model_max = Wav2Vec2ClassificationModel(num_labels=2, hidden_size=768, final_dropout=0.1, pooling = 'max').to(device)\n",
    "model_sum = Wav2Vec2ClassificationModel(num_labels=2, hidden_size=768, final_dropout=0.1, pooling = 'sum').to(device)\n",
    "for model in [model_mean, model_max, model_sum]:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "    print(model.pooling)\n",
    "    model.train_model(train_loader, val_loader, criterion, optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,3,figsize = (12, 8))\n",
    "for ax, model in zip(axes, [model_max, model_mean, model_sum]):\n",
    "    plot_confusion_matrix(model, validation_dataloader, dataset.classes, ax = ax)\n",
    "    ax.set_title(model.pooling)\n",
    "    ax.set_ylabel('')\n",
    "axes[0].set_ylabel('True label')\n",
    "plt.savefig('./media/wav2vec2lr1e3.png', dpi = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### learning rate = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mean = Wav2Vec2ClassificationModel(num_labels=2, hidden_size=768, final_dropout=0.1, pooling = 'mean').to(device)\n",
    "model_max = Wav2Vec2ClassificationModel(num_labels=2, hidden_size=768, final_dropout=0.1, pooling = 'max').to(device)\n",
    "model_sum = Wav2Vec2ClassificationModel(num_labels=2, hidden_size=768, final_dropout=0.1, pooling = 'sum').to(device)\n",
    "for model in [model_mean, model_max, model_sum]:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.00001)\n",
    "    print(model.pooling)\n",
    "    model.train_model(train_loader, val_loader, criterion, optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,3,figsize = (12, 8))\n",
    "for ax, model in zip(axes, [model_max, model_mean, model_sum]):\n",
    "    plot_confusion_matrix(model, val_loader, dataset.classes, ax = ax)\n",
    "    ax.set_title(model.pooling)\n",
    "    ax.set_ylabel('')\n",
    "axes[0].set_ylabel('True label')\n",
    "plt.savefig('./media/wav2vec2lr1e5.png', dpi = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# whole dataset (30 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SoundDataset('./data/train/padded/', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.classes, len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "class_counts = {}\n",
    "for _, label in dataset:\n",
    "    label = label.item()\n",
    "    if label not in class_counts:\n",
    "        class_counts[label] = 0\n",
    "    class_counts[label] += 1\n",
    "\n",
    "# Calculate the desired number of samples for each class in training and validation sets\n",
    "total_samples = len(dataset)\n",
    "train_ratio = 0.8  # Adjust as needed\n",
    "train_class_counts = {label: int(train_ratio * count) for label, count in class_counts.items()}\n",
    "val_class_counts = {label: count - train_class_counts[label] for label, count in class_counts.items()}\n",
    "\n",
    "# Create samplers for training and validation sets while maintaining class balance\n",
    "train_indices = []\n",
    "val_indices = []\n",
    "shuffled_dataset = DataLoader(dataset, shuffle=True).dataset\n",
    "for idx, (_, label) in enumerate(shuffled_dataset):\n",
    "    if train_class_counts[label.item()] > 0:\n",
    "        train_indices.append(idx)\n",
    "        train_class_counts[label.item()] -= 1\n",
    "    else:\n",
    "        val_indices.append(idx)\n",
    "\n",
    "train_dataset = Subset(shuffled_dataset, train_indices)\n",
    "validation_dataset = Subset(shuffled_dataset, val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset), len(validation_dataset), len(dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataloader = DataLoader(validation_dataset, batch_size=64)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model = Wav2Vec2ClassificationModel(num_labels=30, hidden_size=768, final_dropout=0.1, pooling = 'mean').to(device)\n",
    "full_model.load_state_dict(torch.load('./media/full_W2V_epoch3.pth'))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(full_model.parameters(), lr = 1e-5)\n",
    "full_model.train_model(train_dataloader, validation_dataloader, criterion, optimizer, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(full_model.state_dict(), './media/full_W2V_epoch5.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = []\n",
    "predicted_labels = []\n",
    "for batch, labels in validation_dataloader:\n",
    "    predicted_labels+=list(full_model.predict(batch).cpu().numpy())\n",
    "    true_labels+=list(labels.numpy())\n",
    "cm = confusion_matrix(true_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize = (30,30))\n",
    "ConfusionMatrixDisplay(cm, display_labels = dataset.classes).plot(ax = ax, colorbar = False)\n",
    "plt.savefig('./media/wav2vec2full.png', dpi = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag = cm.diagonal()\n",
    "np.fill_diagonal(cm, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize = (30,30))\n",
    "ConfusionMatrixDisplay(cm, display_labels = dataset.classes).plot(ax = ax, colorbar=False)\n",
    "plt.savefig('./media/wav2vec2full_errors.png', dpi = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_dataset.class_to_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = np.array(predicted_labels)\n",
    "true_labels = np.array(true_labels)\n",
    "pred_24 = np.where(predicted_labels == 24)\n",
    "pred_23 = np.where(predicted_labels == 23)\n",
    "true_24 = np.where(true_labels == 24)\n",
    "true_23 = np.where(true_labels == 23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### true three predicted as tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load(shuffled_dataset.paths[np.setdiff1d(true_23, pred_24)[0]])\n",
    "Audio(data=y, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load(shuffled_dataset.paths[np.setdiff1d(true_23, pred_24)[1]])\n",
    "Audio(data=y, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load(shuffled_dataset.paths[np.setdiff1d(true_23, pred_24)[2]])\n",
    "Audio(data=y, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load(shuffled_dataset.paths[np.setdiff1d(true_23, pred_24)[3]])\n",
    "Audio(data=y, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load(shuffled_dataset.paths[np.setdiff1d(true_23, pred_24)[4]])\n",
    "Audio(data=y, rate=16000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CUDA_kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
