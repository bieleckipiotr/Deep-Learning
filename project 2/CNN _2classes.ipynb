{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL CNN - 2 klasy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 4752\n",
      "    Root location: ./data/spectrograms\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(201, 81), interpolation=bilinear, max_size=None, antialias=True)\n",
      "               ToTensor()\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "data_path = './data/spectrograms' #looking in subfolder train\n",
    "\n",
    "yes_no_dataset = datasets.ImageFolder(\n",
    "    root=data_path,\n",
    "    transform=transforms.Compose([transforms.Resize((201,81)),\n",
    "                                  transforms.ToTensor()\n",
    "                                  ])\n",
    ")\n",
    "print(yes_no_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class category and index of the images: {'no': 0, 'yes': 1}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_map=yes_no_dataset.class_to_idx\n",
    "\n",
    "print(\"\\nClass category and index of the images: {}\\n\".format(class_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 3801\n",
      "Testing size: 951\n"
     ]
    }
   ],
   "source": [
    "#split data to test and train\n",
    "#use 80% to train\n",
    "train_size = int(0.8 * len(yes_no_dataset))\n",
    "test_size = len(yes_no_dataset) - train_size\n",
    "yes_no_train_dataset, yes_no_test_dataset = torch.utils.data.random_split(yes_no_dataset, [train_size, test_size])\n",
    "\n",
    "print(\"Training size:\", len(yes_no_train_dataset))\n",
    "print(\"Testing size:\",len(yes_no_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 1922, 0: 1879})"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# labels in training set\n",
    "train_classes = [label for _, label in yes_no_train_dataset]\n",
    "Counter(train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    yes_no_train_dataset,\n",
    "    batch_size=15,\n",
    "    num_workers=2,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    yes_no_test_dataset,\n",
    "    batch_size=15,\n",
    "    num_workers=2,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(51136, 50)\n",
    "        self.fc2 = nn.Linear(50, 2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        #x = x.view(x.size(0), -1)\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.log_softmax(x,dim=1)  \n",
    "\n",
    "model = CNNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost function used to determine best parameters\n",
    "cost = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# used to create optimal parameters\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create the training function\n",
    "\n",
    "def train(dataloader, model, loss, optimizer):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, Y) in enumerate(dataloader):\n",
    "        \n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X)\n",
    "        loss = cost(pred, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f'loss: {loss:>7f}  [{current:>5d}/{size:>5d}]')\n",
    "\n",
    "\n",
    "# Create the validation/test function\n",
    "\n",
    "def test(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, Y) in enumerate(dataloader):\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            pred = model(X)\n",
    "\n",
    "            test_loss += cost(pred, Y).item()\n",
    "            correct += (pred.argmax(1)==Y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "\n",
    "    print(f'\\nTest Error:\\nacc: {(100*correct):>0.1f}%, avg loss: {test_loss:>8f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.693147  [    0/ 3801]\n",
      "loss: 0.541284  [ 1500/ 3801]\n",
      "loss: 0.472459  [ 3000/ 3801]\n",
      "\n",
      "Test Error:\n",
      "acc: 88.2%, avg loss: 0.018088\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.552103  [    0/ 3801]\n",
      "loss: 0.277355  [ 1500/ 3801]\n",
      "loss: 0.357494  [ 3000/ 3801]\n",
      "\n",
      "Test Error:\n",
      "acc: 88.7%, avg loss: 0.017300\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.803141  [    0/ 3801]\n",
      "loss: 0.378211  [ 1500/ 3801]\n",
      "loss: 0.109238  [ 3000/ 3801]\n",
      "\n",
      "Test Error:\n",
      "acc: 90.4%, avg loss: 0.014670\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.287604  [    0/ 3801]\n",
      "loss: 0.252292  [ 1500/ 3801]\n",
      "loss: 0.229924  [ 3000/ 3801]\n",
      "\n",
      "Test Error:\n",
      "acc: 91.0%, avg loss: 0.013492\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.260863  [    0/ 3801]\n",
      "loss: 0.228511  [ 1500/ 3801]\n",
      "loss: 0.312630  [ 3000/ 3801]\n",
      "\n",
      "Test Error:\n",
      "acc: 90.9%, avg loss: 0.012790\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.303294  [    0/ 3801]\n",
      "loss: 0.346964  [ 1500/ 3801]\n",
      "loss: 0.155300  [ 3000/ 3801]\n",
      "\n",
      "Test Error:\n",
      "acc: 91.1%, avg loss: 0.012418\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.189396  [    0/ 3801]\n",
      "loss: 0.102072  [ 1500/ 3801]\n",
      "loss: 0.208520  [ 3000/ 3801]\n",
      "\n",
      "Test Error:\n",
      "acc: 91.7%, avg loss: 0.012647\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.214662  [    0/ 3801]\n",
      "loss: 0.103018  [ 1500/ 3801]\n",
      "loss: 0.202990  [ 3000/ 3801]\n",
      "\n",
      "Test Error:\n",
      "acc: 91.5%, avg loss: 0.011929\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.105721  [    0/ 3801]\n",
      "loss: 0.345606  [ 1500/ 3801]\n",
      "loss: 0.315009  [ 3000/ 3801]\n",
      "\n",
      "Test Error:\n",
      "acc: 92.1%, avg loss: 0.011030\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.122248  [    0/ 3801]\n",
      "loss: 0.266226  [ 1500/ 3801]\n",
      "loss: 0.394829  [ 3000/ 3801]\n",
      "\n",
      "Test Error:\n",
      "acc: 91.9%, avg loss: 0.010778\n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.058545  [    0/ 3801]\n",
      "loss: 0.152279  [ 1500/ 3801]\n",
      "loss: 0.190233  [ 3000/ 3801]\n",
      "\n",
      "Test Error:\n",
      "acc: 92.2%, avg loss: 0.010243\n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.080289  [    0/ 3801]\n",
      "loss: 0.147151  [ 1500/ 3801]\n",
      "loss: 0.157190  [ 3000/ 3801]\n",
      "\n",
      "Test Error:\n",
      "acc: 92.3%, avg loss: 0.010333\n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.123464  [    0/ 3801]\n",
      "loss: 0.210035  [ 1500/ 3801]\n",
      "loss: 0.095912  [ 3000/ 3801]\n",
      "\n",
      "Test Error:\n",
      "acc: 92.6%, avg loss: 0.009573\n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.277479  [    0/ 3801]\n",
      "loss: 0.142412  [ 1500/ 3801]\n",
      "loss: 0.342712  [ 3000/ 3801]\n",
      "\n",
      "Test Error:\n",
      "acc: 92.1%, avg loss: 0.009912\n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.019700  [    0/ 3801]\n",
      "loss: 0.101962  [ 1500/ 3801]\n",
      "loss: 0.097213  [ 3000/ 3801]\n",
      "\n",
      "Test Error:\n",
      "acc: 92.8%, avg loss: 0.009140\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f'Epoch {t+1}\\n-------------------------------')\n",
    "    train(train_dataloader, model, cost, optimizer)\n",
    "    test(test_dataloader, model)\n",
    "print('Done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
